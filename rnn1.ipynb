{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 10)          44221000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 44,293,458\n",
      "Trainable params: 44,293,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras/rnn.ipynb#scrollTo=jfOdaQLhXLDR\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#built-in RNN : tf.kras.layers.SimpleRNN, tf.keras.layers.GRU, tf.keras.layers.LSTM\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=4422100, output_dim=10))\n",
    "\n",
    "#Add a LSTM layer with 128 internal units\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "#Add a Dense layer with 10 units and softmax activation\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 64)          64000     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 256)         247296    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 361,866\n",
      "Trainable params: 361,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "model.add(layers.GRU(256, return_sequences=True))\n",
    "model.add(layers.SimpleRNN(128))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8c53e1914902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/anna/SLR/train_data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'full_filenames' is not defined"
     ]
    }
   ],
   "source": [
    "#1. 데이터 준비\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname) #리스트\n",
    "    full_filenames\n",
    "    for filename in filenames:\n",
    "        full_filenames = os.path.join(dirname, filename) #단어 단위\n",
    "        \n",
    "        '''for full_filename in full_filenames:\n",
    "            full_textfilenames = os.path.join(dirname, full_filname) #텍스트 단위\n",
    "            print(full_textfilenames)\n",
    "        '''\n",
    "        #with open(full_filename,\"r\")\n",
    "          \n",
    "        \n",
    "        \n",
    "search('/Users/anna/SLR/train_data/')\n",
    "print(full_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bright', '.DS_Store', 'Light-blue', 'Bitter', 'Milk', 'Argentina', 'Birthday', 'Enemy', 'Call', 'Deaf', 'Colors', 'Opaque', 'Chewing-gum', 'Find', 'Yellow', 'Name', 'None', 'Buy', 'Country', 'Candy']\n"
     ]
    }
   ],
   "source": [
    "#1. 데이터 준비\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#파일 읽기\n",
    "predict = []\n",
    "with open(\"/Users/anna/SLR/train_data/Argentina/024_001_001.txt\", 'r') as data:\n",
    "    numbers = [[i for i in line.split(' ')] for line in data.readlines()]\n",
    "    numbers.append(\"arhentia\")\n",
    "predict.append(numbers)\n",
    "#print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "with open(\"/Users/anna/SLR/data.pkl\", 'rb') as fin:\n",
    "    frames = pickle.load(fin)\n",
    "    for i, frame in enumerate(frames):\n",
    "        features = frame[0]\n",
    "        word = frame[1]\n",
    "        \n",
    "        X.append(np.array(features))\n",
    "        Y.append(word)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "np.shape(X)\n",
    "\n",
    "\n",
    "(x_train, y_train) = X, Y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3분 케라스 책에 있는 RNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from keras import models, layers\n",
    "\n",
    "from kerasapp import skeras\n",
    "\n",
    "\n",
    "def main():\n",
    "    machine = Machine()\n",
    "    machine.run(epochs=400)\n",
    "    \n",
    "class Machine():\n",
    "    def __init__(self):\n",
    "        self.data = Dataset()\n",
    "        shape = self.data.X.shape[1:]\n",
    "        self.model = rnn_model(shape)\n",
    "        \n",
    "        def run(self, epochs = 400):\n",
    "            d = self.data\n",
    "            X_train, X_test = d.X_train, d.X_test\n",
    "            y_train, y_test = d.y_train, d.y_test\n",
    "            X, y = d.X, d.y\n",
    "            m = self.model\n",
    "            h = m.fit(X_train, y_train, epochs=epochs, validation_data =[X_test, y_test], verbose=0)\n",
    "            \n",
    "            skeras.plot_loss(h)\n",
    "            plt.title(\"History of training\")\n",
    "            plt.show()\n",
    "            \n",
    "            yp = m.predict(X_test)\n",
    "            print('Loss:', m.evalutate(X_test, y_test))\n",
    "            plt.plot(yyp, label='Original')\n",
    "            plt.plot(y_test, label='Prediction')\n",
    "            \n",
    "            plt.legend(loc=0)\n",
    "            plt.title('Validation Results')\n",
    "            plt.show()\n",
    "            \n",
    "            yp = m.predict(X_test).reshape(-1)\n",
    "            print('Loss:', m.evaluate(X_test, y_test))\n",
    "            print(yp.shape, y_test.shape)\n",
    "            \n",
    "            df = pd.DataFrame()\n",
    "            df['Sample'] = list(range(len(y_test))) * 2\n",
    "            df['Normalized #Passengers'] = np.concatenate([y_test, yp], axis=0)\n",
    "            df['Type'] = ['Original'] * len(y_test) + ['Prediction'] + len(yp)\n",
    "            \n",
    "            plt.figure(figsize=(7,5))\n",
    "            sns.barplot(x=\"Sample\", y=\"Normalized #Passengers\", hue=\"Type\", data=df)\n",
    "            plt.ylabel(\"Normalized #Passengers\")\n",
    "            plt.show()\n",
    "            \n",
    "            yp = m.predict(X)\n",
    "            \n",
    "            plt.plot(yp, label='Original')\n",
    "            plt.plot(y, label='Prediction')\n",
    "            plt.legend(loc=0)\n",
    "            plt.title('All Results')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "        def rnn_model(shape):\n",
    "            m_x = layers.Inputs(shape=shape)\n",
    "            m_h = layers.LSTM(10)(m_x)\n",
    "            m_y = layers.Dense(1)(m_h)\n",
    "            m = models.Model(m_x, m_y)\n",
    "            \n",
    "            m.compile('adam', 'mean_squared_error')\n",
    "            \n",
    "            m.summary()\n",
    "            \n",
    "            return m\n",
    "        \n",
    "        \n",
    "        def Dataset:\n",
    "            def __init__(self, frame='international-airline-passengers.csv' D=12)\n",
    "            data_on = load_data(fname=fname)\n",
    "            X, y = get_Xy(data_dn, D=D)\n",
    "            X_train, X_test, y_train, y_test = model.selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "        \n",
    "        def load_data(fname='internatinal-airline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bitter': 1, 'milk': 2, 'argentina': 3, 'deaf': 4, 'chewinggum': 5, 'name': 6, 'none': 7, 'buy': 8, 'country': 9, 'lightblue': 10}\n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "with open(\"/Users/anna/SLR/data.pkl\", 'rb') as fin:\n",
    "    frames = pickle.load(fin)\n",
    "    for i, frame in enumerate(frames):\n",
    "        features = frame[0]\n",
    "        word = frame[1].lower()\n",
    "        \n",
    "        X.append(np.array(features))\n",
    "        Y.append(word)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(Y)\n",
    "print(t.word_index)\n",
    "#Y = to_categorical(Y,len(t.word_index))\n",
    "encoded=t.texts_to_sequences(Y)\n",
    "#print(encoded)\n",
    "one_hot=to_categorical(encoded)\n",
    "print(one_hot)\n",
    "\n",
    "\n",
    "#Y = t.texts_to_sequences(Y)[0]\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stage\n",
      "==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/SLR/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 387s 774ms/step - loss: 0.3023 - accuracy: 0.9091 - val_loss: 0.2980 - val_accuracy: 0.9091\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 380s 760ms/step - loss: 0.2988 - accuracy: 0.9091 - val_loss: 0.2963 - val_accuracy: 0.9091\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 363s 726ms/step - loss: 0.2988 - accuracy: 0.9091 - val_loss: 0.2965 - val_accuracy: 0.9091\n",
      "500/500 [==============================] - 49s 97ms/step\n",
      "Test performance: accuracy=0.9090909361839294, loss=0.29645157051086424\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LSTM example for Keras\n",
    "======================\n",
    "Object oriented style revision of the original code in Keras example\n",
    "- https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py   \n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras import layers, models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        \n",
    "        X = []\n",
    "        Y = []\n",
    "        \n",
    "        with open(\"/Users/anna/SLR/data.pkl\", 'rb') as fin:\n",
    "            frames = pickle.load(fin)\n",
    "            for i, frame in enumerate(frames):\n",
    "                features = frame[0]\n",
    "                word = frame[1]\n",
    "            \n",
    "                X.append(np.array(features))\n",
    "                Y.append(word)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        t = Tokenizer()\n",
    "        t.fit_on_texts(Y)\n",
    "        #print(t.word_index)\n",
    "        #Y = to_categorical(Y,len(t.word_index))\n",
    "        encoded=t.texts_to_sequences(Y)\n",
    "        #print(encoded)\n",
    "        one_hot=to_categorical(encoded)\n",
    "        #print(one_hot)\n",
    "\n",
    "       \n",
    "        (x_train, y_train) = X, one_hot\n",
    "        \n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_train, y_train\n",
    "        \n",
    "        \n",
    "class RNN_LSTM(models.Model):\n",
    "    def __init__(self):\n",
    "        x = layers.Input((8736,))\n",
    "        h = layers.Embedding(8736, 128)(x)\n",
    "        h = layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(h)\n",
    "        y = layers.Dense(11, activation='softmax')(h)\n",
    "        super().__init__(x, y)\n",
    "\n",
    "        # try using different optimizers and different optimizer configs\n",
    "        self.compile(loss='binary_crossentropy',\n",
    "                     optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "class Machine:\n",
    "    def __init__(self):\n",
    "        self.data = Data()\n",
    "        self.model = RNN_LSTM()\n",
    "\n",
    "    def run(self, epochs=3, batch_size=32):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "        print('Training stage')\n",
    "        print('==============')\n",
    "        model.fit(data.x_train, data.y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(data.x_test, data.y_test))\n",
    "\n",
    "        score, acc = model.evaluate(data.x_test, data.y_test,\n",
    "                                    batch_size=batch_size)\n",
    "        print('Test performance: accuracy={0}, loss={1}'.format(acc, score))\n",
    "\n",
    "\n",
    "def main():\n",
    "    m = Machine()\n",
    "    m.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stage\n",
      "==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/SLR/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 411 samples, validate on 411 samples\n",
      "Epoch 1/3\n",
      "411/411 [==============================] - 84s 205ms/step - loss: 0.3455 - accuracy: 0.8889 - val_loss: 0.3392 - val_accuracy: 0.8889\n",
      "Epoch 2/3\n",
      "384/411 [===========================>..] - ETA: 4s - loss: 0.3393 - accuracy: 0.8889 "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LSTM example for Keras\n",
    "======================\n",
    "Object oriented style revision of the original code in Keras example\n",
    "- https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py   \n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras import layers, models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self,pklname):\n",
    "        \n",
    "        X = []\n",
    "        Y = []\n",
    "        maxlength=0\n",
    "        with open(pklname, 'rb') as fin:\n",
    "            frames = pickle.load(fin)\n",
    "            for i, frame in enumerate(frames):\n",
    "                features = frame[0]\n",
    "                maxlength=len(features)\n",
    "                word = frame[1]\n",
    "            \n",
    "                X.append(np.array(features))\n",
    "                Y.append(word)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        t = Tokenizer()\n",
    "        t.fit_on_texts(Y)\n",
    "        #print(t.word_index)\n",
    "        #Y = to_categorical(Y,len(t.word_index))\n",
    "        encoded=t.texts_to_sequences(Y)\n",
    "        #print(encoded)\n",
    "        one_hot=to_categorical(encoded)\n",
    "        #print(one_hot)\n",
    "\n",
    "       \n",
    "        (x_train, y_train) = X, one_hot\n",
    "        \n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_train, y_train\n",
    "        self.length=maxlength\n",
    "        \n",
    "class RNN_LSTM(models.Model):\n",
    "    def __init__(self,maxlen):\n",
    "        x = layers.Input((maxlen,))\n",
    "        h = layers.Embedding(maxlen, 128)(x)\n",
    "        h = layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(h)\n",
    "        y = layers.Dense(9, activation='softmax')(h)\n",
    "        super().__init__(x, y)\n",
    "\n",
    "        # try using different optimizers and different optimizer configs\n",
    "        self.compile(loss='binary_crossentropy',\n",
    "                     optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "class Machine:\n",
    "    def __init__(self,pklname):\n",
    "        self.data = Data(pklname)\n",
    "        self.model = RNN_LSTM(self.data.length)\n",
    "\n",
    "    def run(self, epochs=3, batch_size=32):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "        print('Training stage')\n",
    "        print('==============')\n",
    "        model.fit(data.x_train, data.y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(data.x_test, data.y_test))\n",
    "\n",
    "        score, acc = model.evaluate(data.x_test, data.y_test,\n",
    "                                    batch_size=batch_size)\n",
    "        print('Test performance: accuracy={0}, loss={1}'.format(acc, score))\n",
    "\n",
    "\n",
    "def main(pklname):\n",
    "    m = Machine(pklname)\n",
    "    m.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='run Model')\n",
    "    parser.add_argument(\"--pkl_data_path\",help=\" \")\n",
    "    args=parser.parse_args()\n",
    "    pkl_data_path=args.pkl_data_path\n",
    "    main(pkl_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 10)          42840     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 115,298\n",
      "Trainable params: 115,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#built-in RNN : tf.kras.layers.SimpleRNN, tf.keras.layers.GRU, tf.keras.layers.LSTM\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=8736, output_dim=10)) #최대길이 8736\n",
    "\n",
    "#Add a LSTM layer with 128 internal units\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "#Add a Dense layer with 10 units and softmax activation\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [0,1,2]\n",
    "list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
